apiVersion: apps/v1
kind: Deployment
metadata:
  name: goodlist-vendor-frontend-${ENV_NAME}
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0  # Ensure at least one pod is always running
      maxSurge: 1  # Allow maximum 1 extra pod during updates
  selector:
    matchLabels:
      app: goodlist-vendor-frontend-${ENV_NAME}
  template:
    metadata:
      labels:
        app: goodlist-vendor-frontend-${ENV_NAME}
    spec:
      containers:
        - name: goodlist-vendor-frontend-${ENV_NAME}
          image: goodlist${ENV_ACR}.azurecr.io/goodlist-vendor-frontend-${ENV_NAME}:${IMAGE_TAG}
          imagePullPolicy: Always
          ports:
            - containerPort: 3000
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: agentpool
                operator: In
                values:
                - userpool
# ---
# apiVersion: autoscaling/v2
# kind: HorizontalPodAutoscaler
# metadata:
#   name: goodlist-vendor-frontend-${ENV_NAME}-hpa
# spec:
#   scaleTargetRef:
#     apiVersion: apps/v1
#     kind: Deployment
#     name: goodlist-vendor-frontend-${ENV_NAME}
#   minReplicas: 1
#   maxReplicas: 3  # Adjust based on your workload and resource limits
#   metrics:
#   - type: Resource
#     resource:
#       name: memory
#       target:
#         type: Utilization
#         averageUtilization: 70  # Adjust based on your workload and memory limits
#   - type: Resource
#     resource:
#       name: cpu
#       target:
#         type: Utilization
#         averageUtilization: 50  # Adjust based on your workload and memory limits
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: goodlist-vendor-frontend-${ENV_NAME}-pdb  # You can customize this name
spec:
  minAvailable: 1  # Minimum number of pods to be available at all times
  selector:
    matchLabels:
      app: goodlist-vendor-frontend-${ENV_NAME}  # Label that identifies the pods to protect
  # Alternatively, you can use a selector based on other criteria
  # maxUnavailable: 1  # Optional: Maximum number of pods that can be unavailable during disruption (absolute number or percentage)
---
apiVersion: v1
kind: Service
metadata:
  name: goodlist-vendor-frontend-${ENV_NAME}
spec:
  type: NodePort
  ports:
    - port: 3000
  selector:
    app: goodlist-vendor-frontend-${ENV_NAME}
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: goodlist-vendor-frontend-${ENV_NAME}-ingress
  annotations:
    # nginx.ingress.kubernetes.io/ssl-redirect: "false"
    # nginx.ingress.kubernetes.io/use-regex: "true"
    # nginx.ingress.kubernetes.io/rewrite-target: /$2
    cert-manager.io/cluster-issuer: letsencrypt
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - www.vendor-web.${ENV_NAME}.goodthingco.xyz
        - vendor-web.${ENV_NAME}.goodthingco.xyz
      secretName: tls-secret-vendor-frontend-${ENV_NAME}
  rules:
    - host: www.vendor-web.${ENV_NAME}.goodthingco.xyz
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: goodlist-vendor-frontend-${ENV_NAME}
                port:
                  number: 3000
    - host: vendor-web.${ENV_NAME}.goodthingco.xyz
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: goodlist-vendor-frontend-${ENV_NAME}
                port:
                  number: 3000